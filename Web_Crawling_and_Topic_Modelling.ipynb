{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "364929ff",
   "metadata": {},
   "source": [
    "# Web Crawling and Topic Modelling\n",
    "\n",
    "***\n",
    "\n",
    "Develop a model which will make it possible to identify specific subject matter being discussed/present on web pages by using a combination of web crawling and natural language processing.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd22e04",
   "metadata": {},
   "source": [
    "# 1. Setup environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e53d215",
   "metadata": {},
   "source": [
    "## 1.1 Install relevant libraries\n",
    "\n",
    "Major packages needs to be installed as below:\n",
    "\n",
    "* **Step 1: Install Python 3.9 environment**\n",
    "\n",
    "* **Step 2: Uncomment the code shown blow**\n",
    "\n",
    "* **Step 3: Run the code once to install packages**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77dea0fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install spacy\n",
    "# !pip install scikit-learn\n",
    "# !pip install gensim\n",
    "# !pip install BeautifulSoup\n",
    "# !pip install textrazor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce11b101",
   "metadata": {},
   "source": [
    "## 1.2 Import relevant libraries\n",
    "\n",
    "Major packages used are as follows:\n",
    "\n",
    "* **Natural Language Processing (NLP):** `NLTK`, `spaCy`\n",
    "* **Topic modelling:** `gensim`, `sklearn`\n",
    "* **Web scraping:** `requests`, `BeautifulSoup`, `urllib3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f185aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK package for retreiving stopwords and tokenizer\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# SpaCy package for retreiving stopwords and lemmas\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# String for punctuations\n",
    "import string\n",
    "\n",
    "# RegEx pattern matching library\n",
    "import re\n",
    "\n",
    "# Topic Modelling packages from sklearn: NMF, LDA, SVD\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Use gensim paackage to perform text processing, topic modelling and evaluation\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, ldamodel, tfidfmodel, Nmf\n",
    "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f6d4df",
   "metadata": {},
   "source": [
    "## 2. Web Scraping\n",
    "\n",
    "Perform web scraping on specified URL and retreive the relevant text data such as webpage title, headers, paragraphs and emphasised keywords using `request` and `BeautifulSoup` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e452fd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_by_url(url):\n",
    "    \"\"\"\n",
    "    Retreive html web content from a given URL. \n",
    "    Request is set to timeout after 5s and no TLS cert verification.\n",
    "    When response received successfully, status indicated as 200, else unsucessful.\n",
    "    When URL is inaccessible (error), status indcated as -1 and response as None.\n",
    "    \n",
    "    Returns a tuple of URL name, response status and response content.\n",
    "    \"\"\"\n",
    "    #print(\"retreiving:\", url)\n",
    "    try:\n",
    "        resp = requests.get(\"https://\" + url, params=[(\"q\", f\"{url}\"), (\"u\", \"DEFAULT AGENT\")], timeout=5, verify=False)\n",
    "        if resp and resp.text:\n",
    "            # https://www.restapitutorial.com/httpstatuscodes.html - 200 is successful status\n",
    "            \n",
    "            # print(f\"url:{url}, resp.status_code: {resp.status_code}\")\n",
    "            return url, resp.status_code, resp.text\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    status = -1\n",
    "    response_content = None\n",
    "    return url, status, response_content\n",
    "\n",
    "def extract_content_by_tag(content, tags_list:list):\n",
    "    \"\"\"\n",
    "    Parse the response content retreived from the URL.\n",
    "    \n",
    "    The expected tags are the following:\n",
    "    1. Webpage title: ['title']\n",
    "    2. Webpage headers: ['h1', 'h2', 'h3', 'h4', 'h5']\n",
    "    3. Webpage emphasised keywords: ['b', 'strong']\n",
    "    4. Webpage plain texts: ['p', 'li', 'span']\n",
    "    Other tags will be ignored to avoid irrelavant text data.\n",
    "    \n",
    "    Returns the parsed content as string.\n",
    "    \"\"\"\n",
    "    \n",
    "    parsed_content = BeautifulSoup(content, 'html.parser')\n",
    "    parsed_content = parsed_content.find_all(tags_list)\n",
    "    parsed_content = ' '.join([tag.get_text().strip() for tag in parsed_content])\n",
    "    return parsed_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f439806f",
   "metadata": {},
   "source": [
    "## 3. Text and Token Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64421c7",
   "metadata": {},
   "source": [
    "### 3.1 Text Processing\n",
    "\n",
    "Single-stage text processing combining **(i) tokenization**, **(ii) lemmatization** and **(iii) removal of stopwords, numbers and special characters**.\n",
    "\n",
    "* Stopwords are collected from two libraries, `NLTK` and `spaCy` for comprehensiveness.\n",
    "* Special characters contain punctuations, numbers and manually inputed characters to cover edge cases.\n",
    "* Lemmatization is done by utilising `spaCy`'s text processing capabilities rather than `NLTK` for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b5670a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\haipewang5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "##### Lemmetisation #####\n",
    "# Load spacy dictionary `small English`\n",
    "# Used to parse text and perform lemmatization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "##### Get stopwords from NLTK and SpaCy corpus #####\n",
    "nltk.download('stopwords')\n",
    "stopwords1 = nltk.corpus.stopwords.words(\"english\")\n",
    "stopwords2 = list(STOP_WORDS)\n",
    "stop_words = list(set(stopwords1 + stopwords2))\n",
    "\n",
    "##### Get punctuations and special characters #####\n",
    "punctuations = string.punctuation\n",
    "additional_puncs =  \"â©â£–“”’…\" # add edge cases here\n",
    "punctuations = punctuations + additional_puncs\n",
    "\n",
    "##### RegEx pattern to remove specified characters #####\n",
    "pattern = re.compile(\"[\" + punctuations + \"0-9\" +  \"]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a025c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Lemmatization, tokenization and stopword removal #####\n",
    "def text_preprocess(text):\n",
    "    \"\"\"\n",
    "    Use spaCy's English parser to process text data to retreive text lemmas, \n",
    "    followed by tokenization and removal of stopwords and special characters.\n",
    "    \n",
    "    --- Lemmatization ---\n",
    "    SpaCy will perform Part-of-Speech (POS) tagging behind the scene.\n",
    "    This allows us to retreive the word lemmas based on POS and context. \n",
    "    From orevious testing, it appears that spaCy performs better at lemmatization compared to NLTK's WordNet.\n",
    "    \n",
    "    --- Tokenization & Cleaning ---\n",
    "    Lemmas will be lower-cased and stripped of white spaces. \n",
    "    Any special characters, numbers and punctuations will be removed.\n",
    "    Any words contained in the list of stopwords will be removed.\n",
    "    \n",
    "    Returns cleaned tokenized texts.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.replace(\"_\", \" \")\n",
    "    parser = English()\n",
    "    parsed_doc = parser(nlp(text))\n",
    "    cleaned = [word.lemma_.lower().strip() for word in parsed_doc]\n",
    "    cleaned = [re.sub(r\"[^\\w\\s]\", \" \", re.sub(pattern, \" \", word).strip()) for word in cleaned\n",
    "                                                 if word not in stop_words and word not in list(punctuations)]\n",
    "    cleaned = [word for word in cleaned if len(word) > 1 and len(word) < 15]\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ed0ba7",
   "metadata": {},
   "source": [
    "### 3.2 Token Processing\n",
    "\n",
    "- filter out the URL if the sum number of tokens below specified threshold.\n",
    "\n",
    "- Wegiht the tokens by duplicating for several times to improve the improve the significance of important tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69a8f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_on_tokenlen(df, threshold=0):\n",
    "    # Filters out observations with token length below the threshold\n",
    "    criteria1 = df[\"title_token\"].apply(len) == threshold\n",
    "    criteria2 = df[\"header_token\"].apply(len) == threshold\n",
    "    criteria3 = df[\"body_token\"].apply(len) == threshold\n",
    "    criteria4 = df[\"emph_token\"].apply(len) == threshold\n",
    "\n",
    "    valid_df = df[~ (criteria1 & criteria2 & criteria3 & criteria4)].copy()\n",
    "    empty_df = df[criteria1 & criteria2 & criteria3 & criteria4].copy()\n",
    "    return valid_df, empty_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72a126bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_token(df, weight=2):\n",
    "    # duplicate the important tokens to with a specific weights\n",
    "    df[\"title_token\"] = df[\"title_token\"].apply(lambda x: x*weight)\n",
    "    df[\"header_token\"] = df[\"header_token\"].apply(lambda x: x*weight)\n",
    "    df[\"emph_token\"] = df[\"emph_token\"].apply(lambda x: x*weight)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f777333",
   "metadata": {},
   "source": [
    "## 4. Topic Modelling\n",
    "\n",
    "First bigram tokens are generated and added to the token list before being passed to the model. After building the model and generating the topics, coherence score per topics are generated for topic evaluation.\n",
    "\n",
    "Three approaches will be taken for the topic modelling algorithms as shown below.\n",
    "\n",
    "**Latent Dirichlet Allocation (LDA)**\n",
    "\n",
    "A generative probabilistic model for identifying hidden topics or themes within a large corpus of text documents. The basic idea behind LDA is that each document is composed of a mixture of topics, and each topic is a probability distribution over a fixed vocabulary of words. \n",
    "\n",
    "**Non-negative Matrix Factorization (NMF)**\n",
    "\n",
    "A non-probabilistic, linear-algebraic machine learning algorithm for unsupervised clustering, feature extraction, and dimensionality reduction to decompose a document-term matrix into a product of two non-negative matrices representing topics and their associated word distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aef6ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bigram(tokens_list:list, min_count=1, threshold=0.1,\n",
    "                    lower=2, upper=0.9, keep=5000):\n",
    "    \"\"\"\n",
    "    Models and generates the following:\n",
    "    \n",
    "    --- Bigram Tokens --\n",
    "    Bigrams are word-pairs that creates a different contextual meaning combined.\n",
    "    Using `gensim` bigram phraser, bigram word-pairs are generated and added\n",
    "    to the list of tokens.\n",
    "    \n",
    "    --- Term-Frequency Matrix ---\n",
    "    Dictionary of Word IDs against the respective word frequency is generated.\n",
    "    This matrix is used to train the model. Extreme cases will be removed to\n",
    "    improve efficiency and reduce redundant or extreme words by filtering for\n",
    "    the minimum and maximum word count, and the size of matrix.\n",
    "    \n",
    "    --- Corpus ---\n",
    "    Bag-of-words (BOW) is generated based on the tokens.\n",
    "    \n",
    "    Returns the bigram tokens, tf matrix and corpus\n",
    "    \"\"\"\n",
    "    # Build the bigram phrase model\n",
    "    bigram = Phrases(\n",
    "        tokens_list,\n",
    "        min_count=min_count,\n",
    "        threshold=threshold,\n",
    "        connector_words=ENGLISH_CONNECTOR_WORDS\n",
    "        )\n",
    "    \n",
    "    # Generate a list containing single-word and bigram tokens\n",
    "    bigram_tokens = bigram[tokens_list]\n",
    "    \n",
    "    # Build the id-to-word matrix dictionary\n",
    "    bigram_id2word = corpora.Dictionary(bigram_tokens)\n",
    "    \n",
    "    # Filter out the extreme cases to improve efficiency and accuracy\n",
    "    #bigram_id2word.filter_extremes(\n",
    "    #    no_below=lower, no_above=upper, keep_n=keep    \n",
    "    #)\n",
    "    \n",
    "    # Build the bag-of-word corpus form the tokens\n",
    "    bigram_corpus = [bigram_id2word.doc2bow(word) for word in bigram_tokens]\n",
    "    \n",
    "    return bigram_tokens, bigram_id2word, bigram_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f39f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topics(corpus, id2word, algorithm=\"LDA\", k=6, seed=100):\n",
    "    \"\"\"\n",
    "    Build the model based on the specified algorithm\n",
    "    \n",
    "    --- Latent Dirichlet Allocation ---\n",
    "    Runs LDA algorithm based on word distribution in the topics and\n",
    "    topic distribution in the documents. LDA is a probabilistic model\n",
    "    and it is the most commonly used algorithm.\n",
    "    \n",
    "    --- Non-negative Matrix Factorization ---\n",
    "    Runs NMF algorithm and breaksdown DTM into non-negative topic-word matrix\n",
    "    and topic-document matrix. Unlike LDA, NMF is non-probabilistic and\n",
    "    is based on linear algebra to retreive topics.\n",
    "    \n",
    "    --- Term Frequency - Inverse Document Frequency ---\n",
    "    Calculates TF-IDF of the words in the set of documents. TF-IDF represents\n",
    "    relevance of a word in the corpus.\n",
    "    \n",
    "    Returns the model and the topics generated.\n",
    "    \"\"\"\n",
    "    if algorithm == \"LDA\":\n",
    "        model = ldamodel.LdaModel(\n",
    "            corpus=corpus, \n",
    "            id2word=id2word, \n",
    "            num_topics=k, \n",
    "            random_state=seed,\n",
    "            update_every=1,\n",
    "            chunksize=200,\n",
    "            iterations=100,\n",
    "            passes=10,\n",
    "            alpha='auto',\n",
    "            per_word_topics=True\n",
    "        )\n",
    "    elif algorithm == \"NMF\":\n",
    "        model = Nmf(\n",
    "            corpus=corpus,\n",
    "            id2word=id2word,\n",
    "            num_topics=k,\n",
    "            random_state=seed,\n",
    "            chunksize=200,\n",
    "            passes=10,\n",
    "            kappa=.5,\n",
    "        )\n",
    "    else:\n",
    "        print(\"Model not available. Input: 'LDA', 'NMF'\")\n",
    "        return\n",
    "    \n",
    "    # Store the top-20 words in each topics and the overall probability score\n",
    "    probas={}\n",
    "    counts={}\n",
    "    for i in range(len(model.top_topics(corpus))):\n",
    "        for word in model.top_topics(corpus)[i][0]:\n",
    "            prob = word[0]\n",
    "            term = word[1]\n",
    "            probas[term] = probas.get(term, 0) + prob\n",
    "            counts[term] = counts.get(term, 0) + 1\n",
    "    \n",
    "    # Get the average probability per topics\n",
    "    topics={}\n",
    "    for word, prob in probas.items():\n",
    "        topics[word] = prob / counts[word]\n",
    "    \n",
    "    return model, topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57029008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_coherence(model, tokens, id2word, method='c_v'):\n",
    "    \"\"\"\n",
    "    Build coherence model to evaluate how 'coherent' the keywords are\n",
    "    within the topics. Higher coherence shows well connected words.\n",
    "    \n",
    "    Returns a list of coherence score per topic.\n",
    "    \"\"\"\n",
    "    coherence_model = CoherenceModel(\n",
    "        model=model,\n",
    "        texts=tokens,\n",
    "        dictionary=id2word,\n",
    "        coherence=method\n",
    "    )\n",
    "    \n",
    "    return coherence_model.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e26c5398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_weigh_topics(topics, coherence):\n",
    "    # Sort topics by adjusted probability score in descending order\n",
    "    return {k: (v / coherence) for k, v in sorted(topics.items(), key=lambda x: x[1], reverse=True)}\n",
    "\n",
    "\n",
    "def weighted_average_topics(topics_list):\n",
    "    # Get the weighted average of topics probabilities across token columns\n",
    "    topics = {}\n",
    "    probas={}\n",
    "    counts={}\n",
    "    for topic in topics_list:\n",
    "        for term, prob in topic.items():\n",
    "            probas[term] = probas.get(term, 0) + prob\n",
    "            counts[term] = counts.get(term, 0) + 1\n",
    "\n",
    "    # Get the average probability per topics\n",
    "    topics={}\n",
    "    for word, prob in probas.items():\n",
    "        topics[word] = prob / counts[word]\n",
    "    \n",
    "    return topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51748ed",
   "metadata": {},
   "source": [
    "## 5. Main Pipeline\n",
    "\n",
    "\n",
    "### 5.1 Steps for main function\n",
    "\n",
    "- Stpe 1: Load original input datasets for web crawling and topic modeling (csv file)\n",
    "- Stpe 2: Load previous invalid URL\n",
    "- Stpe 3: Load records of visited URL\n",
    "- Stpe 4: Web scraping and retreive response for each URL\n",
    "- Stpe 5: Text and Token processing to parse raw HTML content\n",
    "- Stpe 6: Topic Modelling\n",
    "- Stpe 7: Results Tagging\n",
    "- Stpe 8: Save and output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863f0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(filepath, model=\"LDA\", mode=\"by_page_url\"):\n",
    "    \"\"\"\n",
    "    The main function to control the whole procedure of web crawling and topic modeling.\n",
    "\n",
    "    Paramerters:\n",
    "    filepath: file path for original input file for web crawling and topic modeling\n",
    "    model: topic model, can take LDA, NMF.\n",
    "    mode: text processing mode, can take by_page_url or by_panelist_id to extract tokens for topic generation\n",
    "    \n",
    "    Return: generates topics for specific model and mode on the input file.\n",
    "    \"\"\"\n",
    "    start_time = time()\n",
    "    \n",
    "    \n",
    "    # 1. Load original input datasets for web crawling and topic modeling\n",
    "    df = pd.read_csv(filepath, encoding=\"utf-8\")\n",
    "    df['PageUrl'] = df['PageUrl'].astype(str)\n",
    "    \n",
    "    \n",
    "    # 2. Load previous invalid URL\n",
    "    # Declate the output file path\n",
    "    output_file_path = \"output_\" + model + \"_\" + mode\n",
    "    if not os.path.exists(output_file_path):\n",
    "        os.makedirs(output_file_path)\n",
    "        \n",
    "    invalid_filename_path = os.path.join(output_file_path, \"invalid_urls.txt\")\n",
    "    if os.path.exists(invalid_filename_path):\n",
    "        with open(invalid_filename_path, 'r', encoding=\"utf-8\") as f:\n",
    "            invalid_urls = f.read().splitlines()\n",
    "    else:\n",
    "        invalid_urls = []\n",
    "    \n",
    "    # Filter dataframe from already known invalid URLs\n",
    "    df_filter = df[~ df['PageUrl'].isin(invalid_urls)].copy()\n",
    "    \n",
    "    \n",
    "    # 3. Load records of visited URL\n",
    "    if mode == \"by_page_url\":\n",
    "        visited_filename_path = os.path.join(output_file_path, \"visited_urls.csv\")\n",
    "        if os.path.exists(visited_filename_path):\n",
    "            visited_urls_df = pd.read_csv(visited_filename_path)\n",
    "\n",
    "            # Fitler out URLs that has been visited and modelled previously\n",
    "            df_filter = df_filter[~df_filter['PageUrl'].isin(visited_urls_df['PageUrl'].values)]\n",
    "\n",
    "            # Get the previously visited and modelled URL for concat later\n",
    "            df_visited = visited_urls_df[visited_urls_df['PageUrl'].isin(df_filter['PageUrl'].values)]\n",
    "\n",
    "        else:\n",
    "            columns = [\"PageUrl\", \"weighted_topics\", \"coherence_model\"]\n",
    "            visited_urls_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "    # 4. Web scraping and retreive response for each URL\n",
    "    df_filter['response'] = df_filter['PageUrl'].apply(lambda x: get_response_by_url(x))\n",
    "    df_filter['response_status'] = df_filter['response'].apply(lambda x: x[1])\n",
    "    df_filter['response_text'] = df_filter['response'].apply(lambda x: x[2])\n",
    "\n",
    "    # Identify new invalid URL, bad response status, exception or response emtpy\n",
    "    invalid_df = df_filter[(df_filter['response_status'] != 200) | (df_filter['response_text'] == \"None\") | (len(df_filter['response_text']) == 0)]\n",
    "    \n",
    "    # Add the newly found invalid URLs due to no response into the main URL list\n",
    "    new_invalid_urls = invalid_df['PageUrl'].unique().tolist()\n",
    "    invalid_urls = list(set(invalid_urls + new_invalid_urls))\n",
    "    \n",
    "    # Filter dataframe from new invalid URLs\n",
    "    df_filter = df_filter[~ df_filter['PageUrl'].isin(new_invalid_urls)]\n",
    "    \n",
    "    \n",
    "    # 5. Text and Token processing to parse raw HTML content\n",
    "    tags = {\n",
    "        'title': ['title'],\n",
    "        'header': ['h1', 'h2', 'h3', 'h4', 'h5'],\n",
    "        'emph': ['green', 'red', 'b', 'strong'],\n",
    "        'body': ['p', 'li', 'span']\n",
    "    }\n",
    "\n",
    "    for key, tag_list in tags.items():\n",
    "        df_filter[key] = df_filter['response_text'].apply(lambda x: extract_content_by_tag(x, tag_list))\n",
    "    \n",
    "    # Text Preprocessing to extract tokens\n",
    "    for key in tags.keys():\n",
    "        df_filter[key + \"_token\"] = df_filter[key].apply(text_preprocess)\n",
    "\n",
    "    # Handeling by_panelist_id mode by merging tokens of same panelist_id into one\n",
    "    if mode == \"by_panelist_id\":\n",
    "        # merge extracted tokens into the same panelist_id for further modeling\n",
    "        df_filter = df_filter.sort_values(by=['panelist_id'])\n",
    "        for idx in list(df_filter['panelist_id'].unique()):\n",
    "            rows = df_filter.loc[df_filter['panelist_id'] == idx].index\n",
    "            if len(rows) > 1:\n",
    "                new_df = df_filter.loc[rows[0]].copy()\n",
    "                temp_all_df = df_filter.loc[rows]\n",
    "                df_filter = df_filter.drop(rows)\n",
    "                new_df['response_text'], new_df['title_token'], new_df['header_token'], new_df['body_token'], new_df['emph_token'] = '', [], [], [], []\n",
    "                for index, one_df in temp_all_df.iterrows():\n",
    "                    new_df['response_text'] += one_df['response_text']\n",
    "                    new_df['title_token'] += list(one_df['title_token'])\n",
    "                    new_df['header_token'] += list(one_df['header_token'])\n",
    "                    new_df['body_token'] += list(one_df['body_token'])\n",
    "                    new_df['emph_token'] += list(one_df['emph_token'])\n",
    "                df_filter.loc[len(df_filter.index)] = new_df\n",
    "\n",
    "\n",
    "    # Token process to weighte the important tokens\n",
    "    df_filter_weighted = weighted_token(df_filter)\n",
    "   \n",
    "    # Remove row if all the token are empty\n",
    "    df_valid, df_empty = filter_on_tokenlen(df_filter_weighted)\n",
    "    \n",
    "    # Add the invalid URLs due to lack of response into the main URL list\n",
    "    insufficient_response_urls = df_empty['PageUrl'].unique().tolist()\n",
    "    invalid_urls = list(set(invalid_urls + insufficient_response_urls))\n",
    "    \n",
    "    # Save the invalid URL list into text file for future usage to avoid executing for previous invalid URL\n",
    "    with open(invalid_filename_path, 'w', encoding=\"utf-8\") as f:\n",
    "        f.write('\\n'.join(invalid_urls))\n",
    "    \n",
    "\n",
    "    # 6. Topic Modelling\n",
    "    if not df_valid.empty:\n",
    "        # a. Bigram modelling\n",
    "        df_valid[\"bigram_model\"] = df_valid.apply(\n",
    "            lambda col: generate_bigram([col['title_token'], col['header_token'], col['body_token'], col['emph_token']]), axis=1)\n",
    "\n",
    "        # b. Topic model algorithm\n",
    "        df_valid[\"topic_model\"] = df_valid[\"bigram_model\"].apply(lambda x: generate_topics(x[2], x[1], model))\n",
    "\n",
    "        # c. Coherence model\n",
    "        df_valid[\"coherence_model\"] = df_valid.apply(lambda col: evaluate_coherence(col[\"topic_model\"][0],col[\"bigram_model\"][0],\n",
    "                    col[\"bigram_model\"][1]), axis=1)\n",
    "        \n",
    "        # d. Sort topics by score\n",
    "        df_valid[\"weighted_topics\"] = df_valid.apply(\n",
    "            lambda col: weighted_average_topics([sort_weigh_topics(col[\"topic_model\"][1], col[\"coherence_model\"])]), axis=1)\n",
    "    else:\n",
    "        # if the URL has been visited before, diectly use the previous topic results\n",
    "        # it is only suitable for \"by_page_url\" mode, because \"by_panelist_url\" mode merge differnt URLs\n",
    "        if mode == \"by_page_url\":\n",
    "            df_valid = df_valid.merge(visited_urls_df, on=\"PageUrl\", how=\"left\")\n",
    "\n",
    "\n",
    "    # 7. Results Tagging\n",
    "    if mode == \"by_page_url\":\n",
    "        columns = [\"PageUrl\", \"weighted_topics\", \"coherence_model\"]\n",
    "        df_tagging = df_valid.loc[:,columns].copy()\n",
    "        \n",
    "        # concate previous visited files if exist, otherwise create a new file\n",
    "        if not os.path.exists(visited_filename_path):\n",
    "            df_tagging.to_csv(visited_filename_path, index=False, encoding=\"utf-8-sig\")\n",
    "        else:\n",
    "            visited_urls_df = pd.concat([visited_urls_df, df_tagging], ignore_index=True)\n",
    "            visited_urls_df.to_csv(visited_filename_path, index=False, encoding=\"utf-8-sig\")\n",
    "            df_tagging = pd.concat([df_tagging, df_visited], ignore_index=True)\n",
    "        \n",
    "        # remove dupliction rows by URLs before merging\n",
    "        df_tagging = df_tagging.drop_duplicates(\"PageUrl\")\n",
    "    elif mode == \"by_panelist_id\":\n",
    "        columns = [\"panelist_id\", \"weighted_topics\", \"coherence_model\"]\n",
    "        df_tagging = df_valid.loc[:,columns].copy()\n",
    "\n",
    "            \n",
    "    # 8. Save and output results\n",
    "    # Tag the original dataset with the results\n",
    "    if mode == \"by_page_url\":\n",
    "        df_output = df.merge(df_tagging, on=\"PageUrl\", how=\"left\")\n",
    "    elif mode == \"by_panelist_id\":\n",
    "        df_output = df.merge(df_tagging, on=\"panelist_id\", how=\"left\")\n",
    "\n",
    "    df_output[\"weighted_topics\"] = df_output[\"weighted_topics\"].fillna(\"N.A.\")\n",
    "    df_output[\"coherence_model\"] = df_output[\"coherence_model\"].fillna(\"N.A.\")\n",
    "\n",
    "    # Save the output file    \n",
    "    output_filename = re.findall(r\"\\\\(.+\\.csv)\", filepath.replace(\".csv\", \"_output.csv\"))[0]\n",
    "    output_path = os.path.join(output_file_path, output_filename)\n",
    "    df_output.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "    \n",
    "    print(\"Run time:\", round(time() - start_time, 4), \"s\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d791d3e8",
   "metadata": {},
   "source": [
    "### 5.2 Execute main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c659aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying folder directory and retreive filepaths to run the script\n",
    "folder = \"sample_URL_95CI_5E\"\n",
    "filepaths = glob(folder + \"\\*.csv\")\n",
    "print(\"No. of files:\", len(filepaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a653a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the script sample filepaths with LDA model and by_page_url mode\n",
    "for file in filepaths:\n",
    "    print(\"Executing model on\", file)\n",
    "    main(file, model=\"LDA\", mode=\"by_page_url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c865bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the script sample filepaths with LDA model and by_panelist_id mode\n",
    "for file in filepaths:\n",
    "    print(\"Executing model on\", file)\n",
    "    main(file, model=\"LDA\", mode=\"by_panelist_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6e645",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the script sample filepaths with NMF model and by_page_url mode\n",
    "for file in filepaths:\n",
    "    print(\"Executing model on\", file)\n",
    "    main(file, model=\"NMF\", mode=\"by_page_url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b4e3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the script sample filepaths with NMF model and by_panelist_id mode\n",
    "for file in filepaths:\n",
    "    print(\"Executing model on\", file)\n",
    "    main(file, model=\"NMF\", mode=\"by_panelist_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2ff6c1",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# 6. Additional: Existing Commercial Tool\n",
    "\n",
    "* Installation and registration to use the existing commercial tool `textrazor`\n",
    "* Calling `textrazor` API and directly model topic from URL\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6c7d0b",
   "metadata": {},
   "source": [
    "### 6.1 Setup TextRazor\n",
    "\n",
    "* Sign up and create your account on [textrazor website](https://www.textrazor.com/)\n",
    "* Validate your email and receive your API key\n",
    "* Install textrazor `pip install textrazor`\n",
    "* _Note: textrazor is not open-source, there is a limit of 500 API calls per API key, do not spam_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baf814e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import textrazor library\n",
    "import textrazor\n",
    "\n",
    "# Input API key received during registration, please don't spam with my API key\n",
    "textrazor.api_key = \"5c2b9b14e0e00b418ea577c64403ef39e6df8717f40a74bb1fb6a6bc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79ab3ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call textrazor client and retreive topic and entity analysis using `analyze_url()`\n",
    "client = textrazor.TextRazor(extractors=[\"entities\", \"topics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2102ca3",
   "metadata": {},
   "source": [
    "### 6.2 Run `textrazor` directly on the given URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f11227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_topics(response, threshold):\n",
    "    # Create list to store the topics and the cut-off threshold score\n",
    "    topics = {}\n",
    "    confidence_threshold = threshold\n",
    "\n",
    "    # Retreive topic labels and score from the model\n",
    "    for topic in response.topics():\n",
    "        if topic.score > confidence_threshold:\n",
    "            topics[topic.label] = topics.get(topic.label, topic.score)\n",
    "\n",
    "        #print(topic.label, topic.score)\n",
    "\n",
    "    # Sort topics based on the confidence score\n",
    "    topics = {k: v for k, v in sorted(topics.items(), key=lambda x: x[1], reverse=True)}\n",
    "    \n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e72df80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_entities(response, threshold):\n",
    "    # Create dictionary to store the topics, entities and the cut-off threshold score\n",
    "    entities = {}\n",
    "    relevance_threshold = threshold\n",
    "\n",
    "    # Retreive entity ids and relevance score from the model\n",
    "    for entity in response.entities():\n",
    "        if (entity.relevance_score > relevance_threshold):\n",
    "            entities[entity.id] = entities.get(entity.id, \n",
    "                                               (entity.relevance_score, entity.confidence_score, entity.freebase_types))\n",
    "\n",
    "            if entities[entity.id][0] < entity.relevance_score:\n",
    "                entities[entity.id] = (entity.relevance_score, entity.confidence_score, entity.freebase_types)\n",
    "\n",
    "        # print(entity.id, entity.relevance_score, entity.confidence_score, entity.freebase_types)\n",
    "\n",
    "    # Sort the entities based on relevance score\n",
    "    entities = {k: v for k, v in sorted(entities.items(), key=lambda x: x[1], reverse=True)}\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "90faf180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of files: 29\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-01_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-02_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-03_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-04_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-05_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-06_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-07_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-08_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-09_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-10_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-11_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-12_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-13_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-14_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-15_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-16_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-18_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-19_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-20_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-21_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-22_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-23_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-24_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-26_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-27_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-28_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-29_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-30_0.csv\n",
      "Executing model on sample_URL_95CI_5E\\sample_2022-12-31_0.csv\n"
     ]
    }
   ],
   "source": [
    "# Run text analysis using textrazor API on the URL\n",
    "# url = \"https://theguardian.com/business/2022/dec/01/big-uk-high-street-bank-slow-react-money-market-movement-fixed-rate-mortgage-savings\"\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "url_response = {}\n",
    "\n",
    "folder = \"sample_URL_95CI_5E\"\n",
    "filepaths = glob(folder + \"\\*.csv\")\n",
    "print(\"No. of files:\", len(filepaths))\n",
    "\n",
    "for filepath in filepaths:\n",
    "    print(\"Executing model on\", filepath)\n",
    "    # randomly pick two links\n",
    "    df = pd.read_csv(filepath, encoding=\"utf-8\")\n",
    "    urls = random.sample(list(df['PageUrl'].astype(str)), k=10)\n",
    "    for url in urls:\n",
    "        response = client.analyze_url(url)\n",
    "        url_response[url] = response\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d402cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive topics and entities based on set threshold\n",
    "\n",
    "results = pd.DataFrame()\n",
    "url_list = []\n",
    "topics_list = []\n",
    "\n",
    "for url in url_response.keys():\n",
    "    topics1 = retreive_topics(url_response[url], 0.2)\n",
    "    entities1 = retreive_entities(url_response[url], 0.5)\n",
    "    \n",
    "    url_list.append(url)\n",
    "    topics_list.append(dict(list(topics1.items())[:10]))\n",
    "    \n",
    "results[\"PageUrl\"] = url_list\n",
    "results[\"weighted_topics\"] = topics_list\n",
    "results.to_csv(\"TextRazor_result.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
